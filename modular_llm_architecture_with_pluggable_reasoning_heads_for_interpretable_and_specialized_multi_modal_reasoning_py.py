# -*- coding: utf-8 -*-
"""Modular LLM Architecture with Pluggable Reasoning Heads for Interpretable and Specialized Multi-modal Reasoning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BV84ACuzqlhAeepGvIjANRGZQCn32F87
"""

!pip install sympy sentence-transformers

from sentence_transformers import SentenceTransformer, util
import sympy
import re

# Load a sentence transformer to help with prompt classification (you can improve this later)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define keywords that suggest symbolic reasoning
symbolic_keywords = [
    "solve", "calculate", "equation", "expression", "derivative", "integral",
    "what is x", "find x", "math", "algebra"
]

def classify_prompt(prompt: str):
    prompt_lower = prompt.lower()

    # Heuristic-based keyword detection (lightweight controller logic)
    for keyword in symbolic_keywords:
        if keyword in prompt_lower:
            return "symbolic"

    # Can be extended with vector similarity using sentence-transformers
    return "base"

from sympy import symbols, Eq, solve, sympify, diff, integrate
import re

x = symbols('x')

def insert_implicit_multiplication(expr: str) -> str:
    # Replace patterns like 4x with 4*x
    expr = re.sub(r'(\d)([a-zA-Z])', r'\1*\2', expr)
    # Replace patterns like x2 with x*2 (optional)
    expr = re.sub(r'([a-zA-Z])(\d)', r'\1*\2', expr)
    return expr

def symbolic_reasoning(prompt: str):
    prompt = prompt.lower().strip().replace('^', '**')
    prompt = re.sub(r'[?]', '', prompt)

    try:
        if "derivative" in prompt:
            expr_match = re.search(r"derivative of (.+)", prompt)
            if not expr_match:
                return "Couldn't find expression to differentiate."
            expr = insert_implicit_multiplication(expr_match.group(1).strip())
            expr_sympy = sympify(expr)
            result = diff(expr_sympy, x)
            return f"The derivative of {expr} is: {result}"

        if "integral" in prompt:
            expr_match = re.search(r"integral of (.+)", prompt)
            if not expr_match:
                return "Couldn't find expression to integrate."
            expr = insert_implicit_multiplication(expr_match.group(1).strip())
            expr_sympy = sympify(expr)
            result = integrate(expr_sympy, x)
            return f"The integral of {expr} is: {result}"

        eq_match = re.search(r"solve (.+)=(.+)", prompt)
        if eq_match:
            left_expr = insert_implicit_multiplication(eq_match.group(1).strip())
            right_expr = insert_implicit_multiplication(eq_match.group(2).strip())
            left = sympify(left_expr)
            right = sympify(right_expr)
            equation = Eq(left, right)
            result = solve(equation, x)
            return f"The solution is: x = {result}"

        return "Sorry, I couldn't parse the symbolic expression."

    except Exception as e:
        return f"Symbolic Reasoning Error: {str(e)}"

def process_prompt(prompt: str):
    task = classify_prompt(prompt)
    print(f"Controller Decision: {task.upper()} HEAD")

    if task == "symbolic":
        return symbolic_reasoning(prompt)
    else:
        return "Use Groq LLM here (not implemented yet)"

test_prompts = [
    "Solve 2*x + 5 = 11",
    "What is the derivative of x^2 + 3*x?",
    "Tell me a story about AI"
]

for p in test_prompts:
    print("\nPrompt:", p)
    print("Output:", process_prompt(p))

!pip install openai

import os
import openai

# Replace YOUR_GROQ_API_KEY with your actual key
os.environ["GROQ_API_KEY"] = "PUT_YOUR_GROQ_API_KEY_HERE"
openai.api_key = os.environ["GROQ_API_KEY"]
openai.api_base = "https://api.groq.com/openai/v1"

from openai import OpenAI

client = OpenAI(
    api_key=os.environ["GROQ_API_KEY"],
    base_url="https://api.groq.com/openai/v1"
)

def call_groq_llm(prompt: str, model="llama-3.3-70b-versatile"):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        return f"[Groq API Error] {str(e)}"

def process_prompt(prompt: str):
    task = classify_prompt(prompt)
    print(f"Controller Decision: {task.upper()} HEAD")

    if task == "symbolic":
        return symbolic_reasoning(prompt)
    else:
        return call_groq_llm(prompt)

test_prompts = [
    "Solve 2*x + 7 = 17",
    "Tell me about the future of artificial intelligence",
    "What is the derivative of x^3 + 4x"
]

for p in test_prompts:
    print("\n---")
    print("Prompt:", p)
    print("Output:", process_prompt(p))

def logical_reasoning(prompt: str):
    try:
        cot_prompt = f"Let's think step by step.\n{prompt}"
        return call_groq_llm(cot_prompt)
    except Exception as e:
        return f"Logical Reasoning Error: {str(e)}"

graph_keywords = ["parent of", "child of", "connected to", "ancestor", "descendant", "graph", "path from"]

def controller(prompt: str) -> str:
    prompt = prompt.lower()

    symbolic_keywords = ["solve", "derivative", "integral", "differentiate", "equation"]
    logic_keywords = ["if", "then", "how many", "more than", "less than", "each", "every", "either", "neither", "both", "logic", "reason", "given"]

    if any(keyword in prompt for keyword in symbolic_keywords):
        return "SYMBOLIC HEAD"
    elif any(keyword in prompt for keyword in logic_keywords):
        return "LOGICAL HEAD"
        # Add after symbolic & logic detection
    elif any(keyword in prompt for keyword in graph_keywords):
        return "GRAPH HEAD"
    else:
        return "BASE HEAD"

def process_prompt(prompt: str) -> str:
    head = controller(prompt)

    if head == "SYMBOLIC HEAD":
        output = symbolic_reasoning(prompt)
    elif head == "LOGICAL HEAD":
        output = logical_reasoning(prompt)
    elif head == "GRAPH HEAD":
        output = graph_reasoning(prompt)
    else:
        output = call_groq_llm(prompt)

    return f"""---
Prompt: {prompt}
Controller Decision: {head}
Output: {output}
---"""

process_prompt("If Alice is taller than Bob, and Bob is taller than Charlie, who is the shortest?")

test_prompts = [
    "Solve 2*x + 7 = 17",
    "Tell me about the future of artificial intelligence",
    "What is the derivative of x^3 + 4x",
    "If Alice is taller than Bob, and Bob is taller than Charlie, who is the shortest?"
    "Alice is the parent of Bob. Bob is the parent of Charlie. Who are the ancestors of Charlie?"
]

for p in test_prompts:
    print("\n---")
    print("Prompt:", p)
    print("Output:", process_prompt(p))

!pip install networkx

import networkx as nx

def graph_reasoning(prompt: str) -> str:
    try:
        # Build a directed graph from the relationships in prompt
        G = nx.DiGraph()

        # Extract basic relationships (e.g., "A is the parent of B")
        relations = re.findall(r'([A-Z][a-z]*) is the (\w+) of ([A-Z][a-z]*)', prompt)
        if not relations:
            return "Graph Reasoning Error: Couldn't extract relationships."

        for subject, relation, obj in relations:
            G.add_edge(subject, obj, relation=relation)

        # Detect reasoning query
        if "ancestor" in prompt:
            person = re.search(r'ancestor of ([A-Z][a-z]*)', prompt)
            if person:
                target = person.group(1)
                ancestors = nx.ancestors(G, target)
                return f"Ancestors of {target}: {', '.join(ancestors) if ancestors else 'None'}"

        if "descendant" in prompt:
            person = re.search(r'descendant of ([A-Z][a-z]*)', prompt)
            if person:
                target = person.group(1)
                descendants = nx.descendants(G, target)
                return f"Descendants of {target}: {', '.join(descendants) if descendants else 'None'}"

        if "path from" in prompt:
            people = re.search(r'path from ([A-Z][a-z]*) to ([A-Z][a-z]*)', prompt)
            if people:
                src, dst = people.groups()
                try:
                    path = nx.shortest_path(G, src, dst)
                    return f"Path from {src} to {dst}: {' → '.join(path)}"
                except nx.NetworkXNoPath:
                    return f"No path from {src} to {dst}."

        return "Graph built, but no reasoning query detected."

    except Exception as e:
        return f"Graph Reasoning Error: {str(e)}"

import json
from datetime import datetime

process_prompt("Alice is the parent of Bob. Bob is the parent of Charlie. Who are the ancestors of Charlie?")

def log_interaction(prompt: str, head: str, output: str, log_file: str = "logs.jsonl"):
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "prompt": prompt,
        "controller_decision": head,
        "output": output
    }
    try:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry) + "\n")
    except Exception as e:
        print(f"Logging Error: {str(e)}")

def process_prompt(prompt: str) -> str:
    head = controller(prompt)

    if head == "SYMBOLIC HEAD":
        output = symbolic_reasoning(prompt)
    elif head == "LOGICAL HEAD":
        output = logical_reasoning(prompt)
    elif head == "GRAPH HEAD":
        output = graph_reasoning(prompt)
    else:
        output = call_groq_llm(prompt)

    # Log interaction
    log_interaction(prompt, head, output)

    return f"""---
Prompt: {prompt}
Controller Decision: {head}
Output: {output}
---"""

test_prompts = [
    "Solve 2*x + 7 = 17",
    "Tell me about the future of artificial intelligence",
    "What is the derivative of x^3 + 4x",
    "If Alice is taller than Bob, and Bob is taller than Charlie, who is the shortest?"
    "Alice is the parent of Bob. Bob is the parent of Charlie. Who are the ancestors of Charlie?"
]

for p in test_prompts:
    print("\n---")
    print("Prompt:", p)
    print("Output:", process_prompt(p))

!cat logs.jsonl

def controller(prompt: str) -> tuple[str, str]:
    symbolic_keywords = ["solve", "equation", "derivative", "integrate", "simplify"]
    logic_keywords = ["if", "then", "must be", "true or false", "all", "some", "none"]
    graph_keywords = ["parent of", "child of", "connected to", "ancestor", "descendant", "path from"]

    for keyword in symbolic_keywords:
        if keyword in prompt.lower():
            return "SYMBOLIC HEAD", f"Matched keyword: '{keyword}' → Symbolic reasoning"

    for keyword in logic_keywords:
        if keyword in prompt.lower():
            return "LOGICAL HEAD", f"Matched keyword: '{keyword}' → Logical reasoning"

    for keyword in graph_keywords:
        if keyword in prompt.lower():
            return "GRAPH HEAD", f"Matched keyword: '{keyword}' → Graph reasoning"

    return "BASE HEAD", "No keyword match → Using base LLM (Groq)"

def process_prompt(prompt: str) -> str:
    head, explanation = controller(prompt)

    if head == "SYMBOLIC HEAD":
        output = symbolic_reasoning(prompt)
    elif head == "LOGICAL HEAD":
        output = logical_reasoning(prompt)
    elif head == "GRAPH HEAD":
        output = graph_reasoning(prompt)
    else:
        output = call_groq_llm(prompt)

    # Log everything (now includes explanation)
    log_interaction(prompt, head, output)

    return f"""---
Prompt: {prompt}
Controller Decision: {head}
Reason: {explanation}
Output: {output}
---"""

print(process_prompt("If it rains, then the ground gets wet. It is raining. Is the ground wet?"))

test_prompts = [
    "Solve 2*x + 7 = 17",
    "Tell me about the future of artificial intelligence",
    "What is the derivative of x^3 + 4x",
    "If Alice is taller than Bob, and Bob is taller than Charlie, who is the shortest?",
    "Alice is the parent of Bob. Bob is the parent of Charlie. Who are the ancestors of Charlie?"
]

for p in test_prompts:
    print("\n---")
    print("Prompt:", p)
    print("Output:", process_prompt(p))

symbolic_keywords = ["solve", "equation", "derivative", "integrate", "simplify"]
logic_keywords = ["if", "then", "must be", "true or false", "all", "some", "none"]
graph_keywords = ["parent of", "child of", "connected to", "ancestor", "descendant", "path from"]

!pip install termcolor

from termcolor import colored  # Install with: pip install termcolor

def highlight_prompt(prompt: str) -> str:
    words = prompt.split()
    highlighted = []

    for word in words:
        lw = word.lower().strip(",.?!")
        if lw in symbolic_keywords:
            highlighted.append(colored(word, "green", attrs=["bold"]))
        elif lw in logic_keywords:
            highlighted.append(colored(word, "blue", attrs=["bold"]))
        elif lw in graph_keywords:
            highlighted.append(colored(word, "magenta", attrs=["bold"]))
        else:
            highlighted.append(word)

    return " ".join(highlighted)

def process_prompt(prompt: str) -> str:
    head, explanation = controller(prompt)

    if head == "SYMBOLIC HEAD":
        output = symbolic_reasoning(prompt)
    elif head == "LOGICAL HEAD":
        output = logical_reasoning(prompt)
    elif head == "GRAPH HEAD":
        output = graph_reasoning(prompt)
    else:
        output = call_groq_llm(prompt)

    # Log it
    log_interaction(prompt, head, output)

    # Highlight prompt tokens
    highlighted = highlight_prompt(prompt)

    return f"""---
🔍 Highlighted Prompt:
{highlighted}

🧠 Controller Decision: {head}
📝 Reason: {explanation}

📤 Output:
{output}
---"""

test_prompts = [
    "Alice is the parent of Bob. Bob is the parent of Charlie. Who are the ancestors of Charlie?",
    "Solve 2*x + 7 = 17",
    "Tell me about the future of artificial intelligence",
    "What is the derivative of x^3 + 4x",
    "If Alice is taller than Bob, and Bob is taller than Charlie, who is the shortest?"
    "Alice is the parent of Bob. Bob is the parent of Charlie. Who are the ancestors of Charlie?"
]

for p in test_prompts:
    print("\n---")
    print("Prompt:", p)
    print("Output:", process_prompt(p))

